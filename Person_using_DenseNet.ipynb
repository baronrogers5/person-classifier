{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Person_using_DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baronrogers5/person-classifier/blob/master/Person_using_DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFzic5lULl5F",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjYlMQl4-CmX",
        "colab_type": "code",
        "outputId": "0032f7d0-4fa6-447e-b4a8-b75f906375f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "import keras\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import os\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00887EBRNT5e",
        "colab_type": "text"
      },
      "source": [
        "## Fetch Data\n",
        "\n",
        "The google drive link contains the person data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfD-Hd6-0yS",
        "colab_type": "code",
        "outputId": "cfdf976b-5276-4c68-8bf0-221550ec45a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKeVOPIQL5jD",
        "colab_type": "text"
      },
      "source": [
        "### Read the annotations file \n",
        "\n",
        "We will be using [pandas](https://pandas.pydata.org/) for reading the input dataframe, and creating one-hot encoded dummy columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS3p__1OO0br",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6a5ad133-f368-4911-b69f-68e66d14aaaa"
      },
      "source": [
        "df = pd.read_csv('hvc_annotations.csv'); df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>images/Set1/5580_2 (3).jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>images/Set1/4650_1 (4).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>images/Set1/44880_0.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>images/Set1/26130_2.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>images/Set1/IMG (4438).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     filename  gender  ...        bodypose     image_path\n",
              "0  images/Set1/5580_2 (3).jpg    male  ...  Front-Frontish  resized/1.jpg\n",
              "1  images/Set1/4650_1 (4).jpg  female  ...  Front-Frontish  resized/2.jpg\n",
              "2     images/Set1/44880_0.jpg    male  ...  Front-Frontish  resized/3.jpg\n",
              "3     images/Set1/26130_2.jpg    male  ...  Front-Frontish  resized/4.jpg\n",
              "4  images/Set1/IMG (4438).jpg  female  ...  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV31PdwtWFto",
        "colab_type": "text"
      },
      "source": [
        "delete the filename column, as it was used for data collection, and does not serve a purpose now. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1WBZlsbO4fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df['filename']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPXhbyyXEod",
        "colab_type": "text"
      },
      "source": [
        "### Data PreProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMHT-3ZWYkg",
        "colab_type": "text"
      },
      "source": [
        "Neural networks expect the prediction labels to be one-hot encoded.\n",
        "So that the predicted probabilities [equal to the number of classes] can be directly matched against the correct labels.\n",
        "\n",
        "Hence, we need to convert all our prediction labels to one-hot encoded with a prefix, so that it becomes easier to identify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA5XhoBi-5he",
        "colab_type": "code",
        "outputId": "2bb60e35-8c88-4767-838e-7b98274e8678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13573, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPhDCJYqdXuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "5416b7b2-e165-44b8-8798-8ab3c6c41378"
      },
      "source": [
        "one_hot_df.head().T"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veYBkbZcV_JB",
        "colab_type": "text"
      },
      "source": [
        "calculate the mean and stddev for normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4AMkbHokYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = np.array([cv2.resize(io.imread(p), (200, 200)) for p in df.iloc[:, -1]])\n",
        "mean = []\n",
        "std = []\n",
        "for i in range(images.shape[-1]):\n",
        "    pixels = images[:, :, :, i].ravel()\n",
        "    mean.append(np.mean(pixels))\n",
        "    std.append(np.std(pixels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CsfzHIOlMF6",
        "colab_type": "code",
        "outputId": "2602fe12-9613-4665-ae2c-9d9bd728ea9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mean, std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([46.56585295255286, 41.46895223605688, 41.246545649451114],\n",
              " [69.21208428987939, 63.936488294726296, 63.29494237674264])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npMwiQrbWq8b",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the one-hot encoded columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKXqT0BaW8Ip",
        "colab_type": "text"
      },
      "source": [
        "Label columns per attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUXz4Dz0W9BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkzsTj4SXL1G",
        "colab_type": "text"
      },
      "source": [
        "The original size of the images were 200x200, and they were resized while sharing the dataset, because the default training and inference were done against vgg16, which requires a (224x224) input. \n",
        "\n",
        "Since, we will be using a [densenet](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803) as our architecture. We don't need the extra computation and hence we resize the images to (200x200).\n",
        "\n",
        "Let's create the keras Sequence which can be called to give a sequence of batches when called by fit_generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PEpjKt8_Qhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, image_size=200, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augmentation = augmentation\n",
        "        self.image_size = image_size\n",
        "        self.normalize_image = lambda x: (x - mean) / std\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        # resizing the image to (self.image_size, self.image_size)\n",
        "        image = np.stack([self.normalize_image(cv2.resize(cv2.imread(item[\"image_path\"]), \n",
        "                                            (self.image_size, self.image_size))) for _, item in items.iterrows()])\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            # This is required for featurewise-center and featurewise-stdnorm\n",
        "            # self.augmentation.fit(image)\n",
        "            image = self.augmentation.flow(image, shuffle=False).next()\n",
        "\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV-PnFNPlrOb",
        "colab_type": "text"
      },
      "source": [
        "### Creating the train and validation data\n",
        "\n",
        "It is important to set a random seed if we are going to be calling fit_generator multiple times, it would prevent train and val data mixing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RV5J00VmZCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=42)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX5UNfhp_TXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YwbHvcGmfWw",
        "colab_type": "text"
      },
      "source": [
        "Let's create insatnces of train and val generators.\n",
        "The train_gen, can be called with get_random_eraser (cutout) as a pre-processing function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBy_y3v2_c31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, image_size=200, augmentation=ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_random_eraser(v_h=1, s_l=0.01, s_h=0.2, r_1=0.1, r_2=1)\n",
        "))\n",
        "\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, image_size=200, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjcS_MYZ_fbn",
        "colab_type": "code",
        "outputId": "6fcae8b5-823f-4410-af80-99c7d067de20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "# num_units actually tells the number of one-hot encoded values / choices for a category\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8qabJ4O_lU3",
        "colab_type": "code",
        "outputId": "b84f333c-b3d8-4581-c89c-20c4f1e346d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# This can act as the measure that val data is always the same\n",
        "val_images, _ = next(iter(valid_gen))\n",
        "plt.imshow(val_images[1]);\n",
        "print()\n",
        "print(f'Train Data Shape: {images[0].shape}, Validation Data Shape: {val_images[0].shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Data Shape: (200, 200, 3), Validation Data Shape: (200, 200, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0jcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReXGzCg8nWqF",
        "colab_type": "text"
      },
      "source": [
        "# Designing the architecture\n",
        "\n",
        "We are not using pre-trained weights and starting from scratch as that was one of the requirements of this exercise.\n",
        "<br>We take a densenet backbone, without the head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyiUt6zYFWLE",
        "colab_type": "text"
      },
      "source": [
        "Since this is a multi-task classification. The final model we are going for would be something like this:\n",
        "\n",
        "\n",
        "```\n",
        "inputs -> \n",
        "        densenet121 + GAP -> (\n",
        "                            tower + gender_head,\n",
        "                            tower + age_head,\n",
        "                            tower + emotion_head, ...\n",
        "                                                    ) -> outputs \n",
        "\n",
        "```\n",
        "1. Choose an architecture as a backbone (here we are choosing densenet121, as it worked better than resnet50 in intial tests), do not include the head, as we would build our own.\n",
        "2. Build towers for each class, the architecture of the towers would largely remain the same.\n",
        "3. Build the respective heads which are the outputs for each class.\n",
        "4. Construct the overall model, specifying the inputs and the outputs.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcDBj4UqILLz",
        "colab_type": "text"
      },
      "source": [
        "#### Constructing the backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1bvSuGvoroI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = DenseNet121(\n",
        "    include_top=False,\n",
        "    input_shape=(200, 200, 3),\n",
        "    weights= None\n",
        ")\n",
        "\n",
        "model = Model(inputs=backbone.input, outputs=backbone.output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt3vK1ACqLHU",
        "colab_type": "text"
      },
      "source": [
        "![backbone head](https://raw.githubusercontent.com/baronrogers5/person-classifier/master/images/backbone_head.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-99MItYyJQ-",
        "colab_type": "text"
      },
      "source": [
        "The final shape after relu is (6, 6, 1024). \n",
        "\n",
        "This is passed through GlobalAveragePooling (further referenced as GAP) to average each feature map across the channel dimension. This brings the tensor shape to a flat (None, 1024).\n",
        "\n",
        " I strongly believe this can be improved if instead of only using GAP, we also use GlobalMaxPooling and concatenate the outputs. That results in 2048 rank 1 tensor, which would have advantages of both mean and max results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqqkt5BpzizZ",
        "colab_type": "text"
      },
      "source": [
        "#### Building the tower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByM8VYorIvbg",
        "colab_type": "text"
      },
      "source": [
        "The tower adds a batchnorm after GAP, to normalize the results after GAP and also a small amount of dropout, to improve resilience.<br> It is followed by a densely connected layer that reduces the nodes to 128, on which each head builds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCSzBpF3IsLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tower(in_layer):\n",
        "    neck = BatchNormalization()(in_layer)\n",
        "    neck = Dropout(0.1)(neck)\n",
        "    neck = Dense(128, activation='relu')(neck)\n",
        "    return neck"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl5VLuMCzlM6",
        "colab_type": "text"
      },
      "source": [
        "The head is the final layer that produces outputs.<br> Let's take pose, there can be 3 possible outputs for it, namely **(front-frontish, back and side)**, so each possible output needs a final node which represents the probability of it's occurrence.<br> `num_units` contains the mapping of each name and the number of categories present. `build_head` builds the heads of each categroy assigning appropriate final nodes as per `num_units`.  \n",
        "\n",
        "```\n",
        "{\n",
        "    'age': 5,\n",
        "    'bag': 3,\n",
        "    'emotion': 4,\n",
        "    'footwear': 3,\n",
        "    'gender': 2,\n",
        "    'image_quality': 3,\n",
        "    'pose': 3,\n",
        "    'weight': 4\n",
        " }\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ylSiYs-Jkh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_head(name, in_layer):\n",
        "    return Dense(num_units[name], activation='softmax', name=f'{name}_output')(in_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2_PKybJJj3t",
        "colab_type": "text"
      },
      "source": [
        "## Code to build the complete Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XpJ3_F8BAig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = DenseNet121(\n",
        "    include_top=False,\n",
        "    input_shape=(200, 200, 3),\n",
        "    weights=None,                     # No pre-trained weights, for imagenet weights, pass `imagenet` as arg. \n",
        ")\n",
        "\n",
        "# \n",
        "neck = GlobalAveragePooling2D()(backbone.output)\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = BatchNormalization()(in_layer)\n",
        "    neck = Dropout(0.1)(neck)\n",
        "    neck = Dense(128, activation='relu')(neck)\n",
        "    return neck\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(num_units[name], activation='softmax', name=f'{name}_output')(in_layer)\n",
        "\n",
        "#heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input,\n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9EZ9CH1CEqJ",
        "colab_type": "code",
        "outputId": "8c052e01-17dc-468c-fc11-edbd8655ef19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 206, 206, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 100, 100, 64) 9408        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 100, 100, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 100, 100, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 102, 102, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 50, 50, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 50, 50, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 50, 50, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 50, 50, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 50, 50, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 50, 50, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 50, 50, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 50, 50, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 50, 50, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 50, 50, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 50, 50, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 50, 50, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 50, 50, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 50, 50, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 50, 50, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 50, 50, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 50, 50, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 50, 50, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 50, 50, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 50, 50, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 50, 50, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 50, 50, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 50, 50, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 50, 50, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 50, 50, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 50, 50, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 50, 50, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 50, 50, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 50, 50, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 50, 50, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 50, 50, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 50, 50, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 50, 50, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 25, 25, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 25, 25, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 25, 25, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 25, 25, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 25, 25, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 25, 25, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 25, 25, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 25, 25, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 25, 25, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 25, 25, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 25, 25, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 25, 25, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 25, 25, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 25, 25, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 25, 25, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 25, 25, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 25, 25, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 25, 25, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 25, 25, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 25, 25, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 25, 25, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 25, 25, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 25, 25, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 25, 25, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 25, 25, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 25, 25, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 25, 25, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 25, 25, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 25, 25, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 25, 25, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 25, 25, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 25, 25, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 25, 25, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 25, 25, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 25, 25, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 25, 25, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 25, 25, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 25, 25, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 25, 25, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 25, 25, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 25, 25, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 25, 25, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 25, 25, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 25, 25, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 25, 25, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 25, 25, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 25, 25, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 25, 25, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 25, 25, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 25, 25, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 25, 25, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 25, 25, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 25, 25, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 25, 25, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 25, 25, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 25, 25, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 25, 25, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 25, 25, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 25, 25, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 25, 25, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 12, 12, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 12, 12, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 12, 12, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 12, 12, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 12, 12, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 12, 12, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 12, 12, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 12, 12, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 12, 12, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 12, 12, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 12, 12, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 12, 12, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 12, 12, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 12, 12, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 12, 12, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 12, 12, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 12, 12, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 12, 12, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 12, 12, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 12, 12, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 12, 12, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 12, 12, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 12, 12, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 12, 12, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 12, 12, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 12, 12, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 12, 12, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 12, 12, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 12, 12, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 12, 12, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 12, 12, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 12, 12, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 12, 12, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 12, 12, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 12, 12, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 12, 12, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 12, 12, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 12, 12, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 12, 12, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 12, 12, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 12, 12, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 12, 12, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 12, 12, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 12, 12, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 12, 12, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 12, 12, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 12, 12, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 12, 12, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 12, 12, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 12, 12, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 12, 12, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 12, 12, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 12, 12, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 12, 12, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 12, 12, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 12, 12, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 12, 12, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 12, 12, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 12, 12, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 12, 12, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 12, 12, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 12, 12, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 12, 12, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 12, 12, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 12, 12, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 12, 12, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 12, 12, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 12, 12, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 12, 12, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 12, 12, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 12, 12, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 12, 12, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 12, 12, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 12, 12, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 12, 12, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 12, 12, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 12, 12, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 12, 12, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 12, 12, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 12, 12, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 12, 12, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 12, 12, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 12, 12, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 12, 12, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 12, 12, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 12, 12, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 12, 12, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 12, 12, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 12, 12, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 12, 12, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 12, 12, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 12, 12, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 12, 12, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 12, 12, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 12, 12, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 12, 12, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 12, 12, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 12, 12, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 12, 12, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 12, 12, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 12, 12, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 12, 12, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 12, 12, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 12, 12, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 12, 12, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 12, 12, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 12, 12, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 12, 12, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 12, 12, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 6, 6, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 6, 6, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 6, 6, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 6, 6, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 6, 6, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 6, 6, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 6, 6, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 6, 6, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 6, 6, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 6, 6, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 6, 6, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 6, 6, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 6, 6, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 6, 6, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 6, 6, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 6, 6, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 6, 6, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 6, 6, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 6, 6, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 6, 6, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 6, 6, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 6, 6, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 6, 6, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 6, 6, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 6, 6, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 6, 6, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 6, 6, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 6, 6, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 6, 6, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 6, 6, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 6, 6, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 6, 6, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 6, 6, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 6, 6, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 6, 6, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 6, 6, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 6, 6, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 6, 6, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 6, 6, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 6, 6, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 6, 6, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 6, 6, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 6, 6, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 6, 6, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 6, 6, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 6, 6, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 6, 6, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 6, 6, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 6, 6, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 6, 6, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 6, 6, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 6, 6, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 6, 6, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 6, 6, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 6, 6, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 6, 6, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 6, 6, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 6, 6, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 6, 6, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 6, 6, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 6, 6, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 6, 6, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 6, 6, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 6, 6, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 6, 6, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 6, 6, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 6, 6, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 6, 6, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 6, 6, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 6, 6, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 6, 6, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 6, 6, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 6, 6, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 6, 6, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 6, 6, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 6, 6, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 1024)         0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 1024)         4096        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          131200      batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          131200      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          131200      batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          131200      batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          131200      batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          131200      batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          131200      batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          131200      batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 8,123,355\n",
            "Trainable params: 8,023,323\n",
            "Non-trainable params: 100,032\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sUg88XRdtn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Attach an image of head\n",
        "plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24aIMWVeRH3w",
        "colab_type": "text"
      },
      "source": [
        "Let's create a function to check whether all the layers are trainable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMkUQBV0Ch2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_all_layers_trainable(model):\n",
        "    # skip the input layer\n",
        "    for l in model.layers[1:]:\n",
        "        assert l.trainable == True, \"Complete Model is not trainable\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrBL0JH2C2S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_all_layers_trainable(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHOqkB2RRwo",
        "colab_type": "text"
      },
      "source": [
        "### Training Callbacks\n",
        "\n",
        "1. <u>`ReduceLROnPlateau`</u> -> To reduce learning_rate when the model val_loss does not improve by `min_delta` for some `patience` epochs\n",
        "2. <u>`ModelCheckpoint`</u> -> To save the model weights, in specified directory.\n",
        "3. <u>`EarlyStopping`</u> -> To stop training if `val_loss` does not improve by `min_delta` for `patience` epochs, restore the `best_weights` during training after stopping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csqzH2w_C-v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduce lr on plateau\n",
        "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.7,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1,\n",
        "    min_delta=1e-3\n",
        ")\n",
        "\n",
        "# save the model\n",
        "save_dir = os.path.join(os.getcwd(), 'gdrive/My Drive/Person_MultiClass')\n",
        "model_name = 'person_multiclass_model_densenet_200_normalized_frozen_reduce_dropout.h5'\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# checkpoint callback\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=8,\n",
        "    verbose=1,\n",
        "    min_delta=1e-3,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6y-GhPjTFtf",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "The model compilation step has been implemented as a function, so that it can be called from any cell, and on any model (backbone or partial models) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ME7MI3TEB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(model):\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=9e-4),\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljo78TUH_w7h",
        "colab_type": "text"
      },
      "source": [
        "### Functions for common use cases "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV3TUcHM_rYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze_model(model: keras.Model):\n",
        "    for l in model.layers:\n",
        "        l.trainable = False\n",
        "\n",
        "def unfreeze_model(model: keras.Model):\n",
        "    for l in model.layers:\n",
        "        l.trainable = True\n",
        "\n",
        "def print_layer_name_and_trainability(model: keras.Model):\n",
        "    for l in model.layers:\n",
        "        print(l.name, l.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePSIXyvT_vnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_results(model: keras.Model, show_loss=False):\n",
        "    data = []\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    for m, r in zip(model.metrics_names, results):\n",
        "        if show_loss:\n",
        "            data.append({'metric_name': m, 'accuracy': r})            \n",
        "        \n",
        "        elif 'acc' in m:\n",
        "                data.append({'metric_name': m, 'accuracy': r})\n",
        "    \n",
        "    df = pd.DataFrame(data, columns=['metric_name', 'accuracy'])\n",
        "    print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u2xPq3DFyzg",
        "colab_type": "text"
      },
      "source": [
        "### Let the training begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9039hG_vF3z-",
        "colab_type": "text"
      },
      "source": [
        "Initial rounds of training were done with a smaller image size of (100x100), so that the model gets to adjust it's initial weights. <br> This helps when data of size (200x200) is introduced, as it acts as completely new data for the model. And, we can do the fine-tuned training with the larger images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsC3QmQLTHtm",
        "colab_type": "code",
        "outputId": "f65302f4-ab2e-46f7-e5bd-900c6d7ceddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "compile_model(model)\n",
        "\n",
        "# for 50 epochs train on (100, 100, 3), with adam defaults, reduce_lr and no data augmentation\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[reduce_lr_on_plateau, checkpoint, early_stopping]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 7.2925 - gender_output_loss: 0.5713 - image_quality_output_loss: 0.9710 - age_output_loss: 1.3900 - weight_output_loss: 0.9666 - bag_output_loss: 0.8790 - footwear_output_loss: 0.8579 - pose_output_loss: 0.7673 - emotion_output_loss: 0.8894 - gender_output_acc: 0.7066 - image_quality_output_acc: 0.5486 - age_output_acc: 0.3987 - weight_output_acc: 0.6335 - bag_output_acc: 0.5918 - footwear_output_acc: 0.6095 - pose_output_acc: 0.6721 - emotion_output_acc: 0.7128 - val_loss: 7.2960 - val_gender_output_loss: 0.5658 - val_image_quality_output_loss: 0.9623 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9596 - val_bag_output_loss: 0.8713 - val_footwear_output_loss: 0.8495 - val_pose_output_loss: 0.7811 - val_emotion_output_loss: 0.9026 - val_gender_output_acc: 0.7072 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6109 - val_pose_output_acc: 0.6628 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.29604, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 7.1913 - gender_output_loss: 0.5403 - image_quality_output_loss: 0.9678 - age_output_loss: 1.3853 - weight_output_loss: 0.9661 - bag_output_loss: 0.8721 - footwear_output_loss: 0.8359 - pose_output_loss: 0.7339 - emotion_output_loss: 0.8900 - gender_output_acc: 0.7266 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3982 - weight_output_acc: 0.6333 - bag_output_acc: 0.6018 - footwear_output_acc: 0.6186 - pose_output_acc: 0.6861 - emotion_output_acc: 0.7122 - val_loss: 7.3698 - val_gender_output_loss: 0.5781 - val_image_quality_output_loss: 0.9671 - val_age_output_loss: 1.4335 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8849 - val_footwear_output_loss: 0.8510 - val_pose_output_loss: 0.7633 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.6915 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6326 - val_pose_output_acc: 0.6623 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 7.29604\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 7.1525 - gender_output_loss: 0.5282 - image_quality_output_loss: 0.9675 - age_output_loss: 1.3897 - weight_output_loss: 0.9670 - bag_output_loss: 0.8691 - footwear_output_loss: 0.8252 - pose_output_loss: 0.7172 - emotion_output_loss: 0.8886 - gender_output_acc: 0.7326 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4019 - weight_output_acc: 0.6337 - bag_output_acc: 0.6032 - footwear_output_acc: 0.6294 - pose_output_acc: 0.6952 - emotion_output_acc: 0.7125 - val_loss: 7.3651 - val_gender_output_loss: 0.5442 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8825 - val_footwear_output_loss: 0.8623 - val_pose_output_loss: 0.7780 - val_emotion_output_loss: 0.9361 - val_gender_output_acc: 0.7228 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6144 - val_pose_output_acc: 0.6830 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 7.29604\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 7.0145 - gender_output_loss: 0.4990 - image_quality_output_loss: 0.9579 - age_output_loss: 1.3716 - weight_output_loss: 0.9583 - bag_output_loss: 0.8605 - footwear_output_loss: 0.8046 - pose_output_loss: 0.6851 - emotion_output_loss: 0.8774 - gender_output_acc: 0.7530 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4038 - weight_output_acc: 0.6328 - bag_output_acc: 0.6130 - footwear_output_acc: 0.6413 - pose_output_acc: 0.7121 - emotion_output_acc: 0.7126 - val_loss: 7.1258 - val_gender_output_loss: 0.5140 - val_image_quality_output_loss: 0.9592 - val_age_output_loss: 1.3896 - val_weight_output_loss: 0.9623 - val_bag_output_loss: 0.8597 - val_footwear_output_loss: 0.8257 - val_pose_output_loss: 0.7083 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.7525 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6109 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.6991 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.29604 to 7.12577, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1.h5\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 6.9595 - gender_output_loss: 0.4867 - image_quality_output_loss: 0.9568 - age_output_loss: 1.3657 - weight_output_loss: 0.9544 - bag_output_loss: 0.8585 - footwear_output_loss: 0.8005 - pose_output_loss: 0.6631 - emotion_output_loss: 0.8737 - gender_output_acc: 0.7614 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4068 - weight_output_acc: 0.6329 - bag_output_acc: 0.6114 - footwear_output_acc: 0.6456 - pose_output_acc: 0.7247 - emotion_output_acc: 0.7124 - val_loss: 7.5892 - val_gender_output_loss: 0.6154 - val_image_quality_output_loss: 1.0183 - val_age_output_loss: 1.4328 - val_weight_output_loss: 1.0298 - val_bag_output_loss: 0.8969 - val_footwear_output_loss: 0.8427 - val_pose_output_loss: 0.7670 - val_emotion_output_loss: 0.9864 - val_gender_output_acc: 0.6875 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3916 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.6779 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.12577\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 6.8758 - gender_output_loss: 0.4644 - image_quality_output_loss: 0.9525 - age_output_loss: 1.3627 - weight_output_loss: 0.9491 - bag_output_loss: 0.8510 - footwear_output_loss: 0.7852 - pose_output_loss: 0.6432 - emotion_output_loss: 0.8678 - gender_output_acc: 0.7777 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4088 - weight_output_acc: 0.6359 - bag_output_acc: 0.6142 - footwear_output_acc: 0.6516 - pose_output_acc: 0.7295 - emotion_output_acc: 0.7130 - val_loss: 7.6595 - val_gender_output_loss: 0.5415 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.4036 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.8645 - val_footwear_output_loss: 0.8251 - val_pose_output_loss: 1.0502 - val_emotion_output_loss: 1.0062 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5973 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.5383 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.12577\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 6.8611 - gender_output_loss: 0.4536 - image_quality_output_loss: 0.9510 - age_output_loss: 1.3605 - weight_output_loss: 0.9495 - bag_output_loss: 0.8475 - footwear_output_loss: 0.7881 - pose_output_loss: 0.6384 - emotion_output_loss: 0.8725 - gender_output_acc: 0.7863 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4127 - weight_output_acc: 0.6345 - bag_output_acc: 0.6153 - footwear_output_acc: 0.6528 - pose_output_acc: 0.7370 - emotion_output_acc: 0.7126 - val_loss: 7.7059 - val_gender_output_loss: 0.7761 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4626 - val_weight_output_loss: 1.0501 - val_bag_output_loss: 0.9176 - val_footwear_output_loss: 0.8624 - val_pose_output_loss: 0.7029 - val_emotion_output_loss: 0.9580 - val_gender_output_acc: 0.5917 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3886 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5413 - val_footwear_output_acc: 0.6260 - val_pose_output_acc: 0.7142 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 7.12577\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 6.6830 - gender_output_loss: 0.4191 - image_quality_output_loss: 0.9403 - age_output_loss: 1.3445 - weight_output_loss: 0.9377 - bag_output_loss: 0.8327 - footwear_output_loss: 0.7657 - pose_output_loss: 0.5878 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8058 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4168 - weight_output_acc: 0.6368 - bag_output_acc: 0.6305 - footwear_output_acc: 0.6616 - pose_output_acc: 0.7553 - emotion_output_acc: 0.7127 - val_loss: 7.0211 - val_gender_output_loss: 0.4578 - val_image_quality_output_loss: 0.9510 - val_age_output_loss: 1.3780 - val_weight_output_loss: 0.9706 - val_bag_output_loss: 0.8532 - val_footwear_output_loss: 0.7970 - val_pose_output_loss: 0.6834 - val_emotion_output_loss: 0.9301 - val_gender_output_acc: 0.7807 - val_image_quality_output_acc: 0.5665 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6058 - val_footwear_output_acc: 0.6502 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.12577 to 7.02114, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1.h5\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 6.6055 - gender_output_loss: 0.4003 - image_quality_output_loss: 0.9364 - age_output_loss: 1.3372 - weight_output_loss: 0.9342 - bag_output_loss: 0.8259 - footwear_output_loss: 0.7516 - pose_output_loss: 0.5665 - emotion_output_loss: 0.8533 - gender_output_acc: 0.8158 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4186 - weight_output_acc: 0.6362 - bag_output_acc: 0.6271 - footwear_output_acc: 0.6694 - pose_output_acc: 0.7661 - emotion_output_acc: 0.7124 - val_loss: 7.2655 - val_gender_output_loss: 0.5175 - val_image_quality_output_loss: 0.9897 - val_age_output_loss: 1.4294 - val_weight_output_loss: 1.0012 - val_bag_output_loss: 0.8661 - val_footwear_output_loss: 0.8316 - val_pose_output_loss: 0.6845 - val_emotion_output_loss: 0.9455 - val_gender_output_acc: 0.7641 - val_image_quality_output_acc: 0.5716 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.7379 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 7.02114\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 6.5023 - gender_output_loss: 0.3752 - image_quality_output_loss: 0.9279 - age_output_loss: 1.3294 - weight_output_loss: 0.9273 - bag_output_loss: 0.8109 - footwear_output_loss: 0.7374 - pose_output_loss: 0.5475 - emotion_output_loss: 0.8466 - gender_output_acc: 0.8332 - image_quality_output_acc: 0.5589 - age_output_acc: 0.4224 - weight_output_acc: 0.6389 - bag_output_acc: 0.6420 - footwear_output_acc: 0.6760 - pose_output_acc: 0.7748 - emotion_output_acc: 0.7127 - val_loss: 6.9808 - val_gender_output_loss: 0.4485 - val_image_quality_output_loss: 0.9538 - val_age_output_loss: 1.3713 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.8404 - val_footwear_output_loss: 0.8147 - val_pose_output_loss: 0.6512 - val_emotion_output_loss: 0.9224 - val_gender_output_acc: 0.7888 - val_image_quality_output_acc: 0.5464 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6502 - val_pose_output_acc: 0.7344 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.02114 to 6.98077, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1.h5\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 6.4340 - gender_output_loss: 0.3616 - image_quality_output_loss: 0.9255 - age_output_loss: 1.3225 - weight_output_loss: 0.9221 - bag_output_loss: 0.8074 - footwear_output_loss: 0.7309 - pose_output_loss: 0.5226 - emotion_output_loss: 0.8414 - gender_output_acc: 0.8391 - image_quality_output_acc: 0.5612 - age_output_acc: 0.4209 - weight_output_acc: 0.6382 - bag_output_acc: 0.6457 - footwear_output_acc: 0.6831 - pose_output_acc: 0.7856 - emotion_output_acc: 0.7129 - val_loss: 7.0635 - val_gender_output_loss: 0.5010 - val_image_quality_output_loss: 0.9562 - val_age_output_loss: 1.3663 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.8556 - val_footwear_output_loss: 0.8016 - val_pose_output_loss: 0.6861 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.7560 - val_image_quality_output_acc: 0.5701 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6149 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7238 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.98077\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 6.3264 - gender_output_loss: 0.3383 - image_quality_output_loss: 0.9189 - age_output_loss: 1.3081 - weight_output_loss: 0.9142 - bag_output_loss: 0.7951 - footwear_output_loss: 0.7184 - pose_output_loss: 0.5011 - emotion_output_loss: 0.8324 - gender_output_acc: 0.8538 - image_quality_output_acc: 0.5618 - age_output_acc: 0.4254 - weight_output_acc: 0.6401 - bag_output_acc: 0.6501 - footwear_output_acc: 0.6880 - pose_output_acc: 0.7945 - emotion_output_acc: 0.7125 - val_loss: 7.2305 - val_gender_output_loss: 0.4955 - val_image_quality_output_loss: 0.9677 - val_age_output_loss: 1.4019 - val_weight_output_loss: 1.0279 - val_bag_output_loss: 0.9123 - val_footwear_output_loss: 0.8089 - val_pose_output_loss: 0.6528 - val_emotion_output_loss: 0.9635 - val_gender_output_acc: 0.7762 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7445 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.98077\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 6.2596 - gender_output_loss: 0.3248 - image_quality_output_loss: 0.9181 - age_output_loss: 1.2981 - weight_output_loss: 0.9073 - bag_output_loss: 0.7839 - footwear_output_loss: 0.7111 - pose_output_loss: 0.4833 - emotion_output_loss: 0.8331 - gender_output_acc: 0.8574 - image_quality_output_acc: 0.5653 - age_output_acc: 0.4324 - weight_output_acc: 0.6430 - bag_output_acc: 0.6560 - footwear_output_acc: 0.6906 - pose_output_acc: 0.8003 - emotion_output_acc: 0.7131 - val_loss: 7.3271 - val_gender_output_loss: 0.5621 - val_image_quality_output_loss: 0.9680 - val_age_output_loss: 1.4347 - val_weight_output_loss: 0.9951 - val_bag_output_loss: 0.9147 - val_footwear_output_loss: 0.8274 - val_pose_output_loss: 0.6645 - val_emotion_output_loss: 0.9607 - val_gender_output_acc: 0.7505 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3856 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5701 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.7455 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.98077\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 6.98077\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 6.0256 - gender_output_loss: 0.2824 - image_quality_output_loss: 0.9033 - age_output_loss: 1.2692 - weight_output_loss: 0.8895 - bag_output_loss: 0.7620 - footwear_output_loss: 0.6785 - pose_output_loss: 0.4292 - emotion_output_loss: 0.8114 - gender_output_acc: 0.8847 - image_quality_output_acc: 0.5701 - age_output_acc: 0.4429 - weight_output_acc: 0.6439 - bag_output_acc: 0.6730 - footwear_output_acc: 0.7087 - pose_output_acc: 0.8263 - emotion_output_acc: 0.7139 - val_loss: 7.5259 - val_gender_output_loss: 0.6614 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4375 - val_weight_output_loss: 1.0033 - val_bag_output_loss: 0.8723 - val_footwear_output_loss: 0.8560 - val_pose_output_loss: 0.7317 - val_emotion_output_loss: 0.9735 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6179 - val_pose_output_acc: 0.7475 - val_emotion_output_acc: 0.7056\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 6.98077\n",
            "Epoch 15/50\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 5.8923 - gender_output_loss: 0.2540 - image_quality_output_loss: 0.8949 - age_output_loss: 1.2497 - weight_output_loss: 0.8677 - bag_output_loss: 0.7460 - footwear_output_loss: 0.6699 - pose_output_loss: 0.4079 - emotion_output_loss: 0.8023 - gender_output_acc: 0.8940 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4513 - weight_output_acc: 0.6451 - bag_output_acc: 0.6809 - footwear_output_acc: 0.7114 - pose_output_acc: 0.8385 - emotion_output_acc: 0.7127 - val_loss: 7.2774 - val_gender_output_loss: 0.5146 - val_image_quality_output_loss: 0.9949 - val_age_output_loss: 1.3970 - val_weight_output_loss: 1.0011 - val_bag_output_loss: 0.8541 - val_footwear_output_loss: 0.8390 - val_pose_output_loss: 0.7043 - val_emotion_output_loss: 0.9724 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7419 - val_emotion_output_acc: 0.7051\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.98077\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 5.7646 - gender_output_loss: 0.2383 - image_quality_output_loss: 0.8852 - age_output_loss: 1.2333 - weight_output_loss: 0.8631 - bag_output_loss: 0.7330 - footwear_output_loss: 0.6488 - pose_output_loss: 0.3658 - emotion_output_loss: 0.7971 - gender_output_acc: 0.9023 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4592 - weight_output_acc: 0.6516 - bag_output_acc: 0.6854 - footwear_output_acc: 0.7197 - pose_output_acc: 0.8544 - emotion_output_acc: 0.7155 - val_loss: 7.6035 - val_gender_output_loss: 0.6293 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.4281 - val_weight_output_loss: 1.0254 - val_bag_output_loss: 0.8732 - val_footwear_output_loss: 0.8996 - val_pose_output_loss: 0.8149 - val_emotion_output_loss: 0.9484 - val_gender_output_acc: 0.7999 - val_image_quality_output_acc: 0.5736 - val_age_output_acc: 0.3896 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.6179 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.7389 - val_emotion_output_acc: 0.7046\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 6.98077\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00016: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d1063fba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twVArpncDPBq",
        "colab_type": "text"
      },
      "source": [
        "Save the trained model as  - `person_multiclass_model_densenet_100_v1.h5`.<br>\n",
        "Now let's freeze the initial layers(backbone), and train a bit further.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcAnNZiQTrxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in model.layers[:427]:\n",
        "    l.trainable = False \n",
        "\n",
        "for i, l in enumerate(model.layers):\n",
        "    print(i, l, l.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gwioTrOfsQN",
        "colab_type": "code",
        "outputId": "b15e6703-ac0b-4e61-8022-241d0108a422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "compile_model(model)\n",
        "\n",
        "# for 50 epochs train on (100, 100, 3), with adam defaults, reduce_lr and no data augmentation and frozen initial layers\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[reduce_lr_on_plateau, checkpoint, early_stopping]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.5417 - gender_output_loss: 0.3618 - image_quality_output_loss: 0.9443 - age_output_loss: 1.3463 - weight_output_loss: 0.9387 - bag_output_loss: 0.8155 - footwear_output_loss: 0.7401 - pose_output_loss: 0.5290 - emotion_output_loss: 0.8659 - gender_output_acc: 0.8461 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4127 - weight_output_acc: 0.6374 - bag_output_acc: 0.6438 - footwear_output_acc: 0.6800 - pose_output_acc: 0.7869 - emotion_output_acc: 0.7099\n",
            "360/360 [==============================] - 58s 162ms/step - loss: 6.5428 - gender_output_loss: 0.3617 - image_quality_output_loss: 0.9443 - age_output_loss: 1.3462 - weight_output_loss: 0.9398 - bag_output_loss: 0.8157 - footwear_output_loss: 0.7407 - pose_output_loss: 0.5292 - emotion_output_loss: 0.8653 - gender_output_acc: 0.8460 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4130 - weight_output_acc: 0.6369 - bag_output_acc: 0.6438 - footwear_output_acc: 0.6798 - pose_output_acc: 0.7866 - emotion_output_acc: 0.7101 - val_loss: 7.1298 - val_gender_output_loss: 0.5135 - val_image_quality_output_loss: 0.9556 - val_age_output_loss: 1.3717 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8501 - val_footwear_output_loss: 0.8493 - val_pose_output_loss: 0.6608 - val_emotion_output_loss: 0.9418 - val_gender_output_acc: 0.7807 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.6351 - val_footwear_output_acc: 0.6326 - val_pose_output_acc: 0.7470 - val_emotion_output_acc: 0.7061\n",
            "Epoch 1/50\n",
            "Epoch 00001: val_loss improved from inf to 7.12982, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1_frozen.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 31s 87ms/step - loss: 6.4780 - gender_output_loss: 0.3506 - image_quality_output_loss: 0.9349 - age_output_loss: 1.3356 - weight_output_loss: 0.9371 - bag_output_loss: 0.8105 - footwear_output_loss: 0.7338 - pose_output_loss: 0.5197 - emotion_output_loss: 0.8559 - gender_output_acc: 0.8478 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4099 - weight_output_acc: 0.6356 - bag_output_acc: 0.6489 - footwear_output_acc: 0.6823 - pose_output_acc: 0.7852 - emotion_output_acc: 0.7124 - val_loss: 7.0601 - val_gender_output_loss: 0.4365 - val_image_quality_output_loss: 0.9671 - val_age_output_loss: 1.3904 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8446 - val_footwear_output_loss: 0.8234 - val_pose_output_loss: 0.6812 - val_emotion_output_loss: 0.9406 - val_gender_output_acc: 0.8034 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.6331 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7409 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.12982 to 7.06014, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1_frozen.h5\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 31s 86ms/step - loss: 6.4251 - gender_output_loss: 0.3475 - image_quality_output_loss: 0.9334 - age_output_loss: 1.3249 - weight_output_loss: 0.9259 - bag_output_loss: 0.8028 - footwear_output_loss: 0.7282 - pose_output_loss: 0.5107 - emotion_output_loss: 0.8516 - gender_output_acc: 0.8527 - image_quality_output_acc: 0.5595 - age_output_acc: 0.4232 - weight_output_acc: 0.6395 - bag_output_acc: 0.6504 - footwear_output_acc: 0.6840 - pose_output_acc: 0.7910 - emotion_output_acc: 0.7126 - val_loss: 7.1180 - val_gender_output_loss: 0.4732 - val_image_quality_output_loss: 1.0006 - val_age_output_loss: 1.3838 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8497 - val_footwear_output_loss: 0.8240 - val_pose_output_loss: 0.6852 - val_emotion_output_loss: 0.9184 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6295 - val_footwear_output_acc: 0.6532 - val_pose_output_acc: 0.7324 - val_emotion_output_acc: 0.7077\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 7.06014\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 33s 91ms/step - loss: 6.3964 - gender_output_loss: 0.3475 - image_quality_output_loss: 0.9300 - age_output_loss: 1.3205 - weight_output_loss: 0.9218 - bag_output_loss: 0.7997 - footwear_output_loss: 0.7229 - pose_output_loss: 0.5072 - emotion_output_loss: 0.8468 - gender_output_acc: 0.8493 - image_quality_output_acc: 0.5591 - age_output_acc: 0.4219 - weight_output_acc: 0.6394 - bag_output_acc: 0.6510 - footwear_output_acc: 0.6907 - pose_output_acc: 0.7916 - emotion_output_acc: 0.7118 - val_loss: 7.2197 - val_gender_output_loss: 0.4322 - val_image_quality_output_loss: 0.9755 - val_age_output_loss: 1.3823 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.8729 - val_footwear_output_loss: 0.8884 - val_pose_output_loss: 0.7529 - val_emotion_output_loss: 0.9499 - val_gender_output_acc: 0.7923 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.6179 - val_footwear_output_acc: 0.6381 - val_pose_output_acc: 0.7238 - val_emotion_output_acc: 0.7051\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.06014\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 32s 88ms/step - loss: 6.3805 - gender_output_loss: 0.3430 - image_quality_output_loss: 0.9282 - age_output_loss: 1.3163 - weight_output_loss: 0.9230 - bag_output_loss: 0.7952 - footwear_output_loss: 0.7244 - pose_output_loss: 0.5041 - emotion_output_loss: 0.8465 - gender_output_acc: 0.8515 - image_quality_output_acc: 0.5600 - age_output_acc: 0.4201 - weight_output_acc: 0.6387 - bag_output_acc: 0.6531 - footwear_output_acc: 0.6872 - pose_output_acc: 0.7926 - emotion_output_acc: 0.7114 - val_loss: 7.1234 - val_gender_output_loss: 0.5064 - val_image_quality_output_loss: 0.9527 - val_age_output_loss: 1.3871 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.8479 - val_footwear_output_loss: 0.8377 - val_pose_output_loss: 0.6755 - val_emotion_output_loss: 0.9298 - val_gender_output_acc: 0.7823 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.3886 - val_weight_output_acc: 0.6321 - val_bag_output_acc: 0.6240 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7424 - val_emotion_output_acc: 0.6991\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.06014\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 31s 86ms/step - loss: 6.3111 - gender_output_loss: 0.3398 - image_quality_output_loss: 0.9208 - age_output_loss: 1.3063 - weight_output_loss: 0.9108 - bag_output_loss: 0.7892 - footwear_output_loss: 0.7141 - pose_output_loss: 0.4967 - emotion_output_loss: 0.8335 - gender_output_acc: 0.8533 - image_quality_output_acc: 0.5593 - age_output_acc: 0.4312 - weight_output_acc: 0.6384 - bag_output_acc: 0.6546 - footwear_output_acc: 0.6895 - pose_output_acc: 0.7997 - emotion_output_acc: 0.7128 - val_loss: 7.0783 - val_gender_output_loss: 0.4721 - val_image_quality_output_loss: 0.9651 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9698 - val_bag_output_loss: 0.8527 - val_footwear_output_loss: 0.8341 - val_pose_output_loss: 0.6737 - val_emotion_output_loss: 0.9271 - val_gender_output_acc: 0.7893 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.6240 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7344 - val_emotion_output_acc: 0.7046\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.06014\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 32s 90ms/step - loss: 6.2736 - gender_output_loss: 0.3387 - image_quality_output_loss: 0.9191 - age_output_loss: 1.2968 - weight_output_loss: 0.9059 - bag_output_loss: 0.7849 - footwear_output_loss: 0.7132 - pose_output_loss: 0.4866 - emotion_output_loss: 0.8283 - gender_output_acc: 0.8569 - image_quality_output_acc: 0.5627 - age_output_acc: 0.4324 - weight_output_acc: 0.6438 - bag_output_acc: 0.6612 - footwear_output_acc: 0.6885 - pose_output_acc: 0.8002 - emotion_output_acc: 0.7119 - val_loss: 7.1558 - val_gender_output_loss: 0.4806 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.3909 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8539 - val_footwear_output_loss: 0.8650 - val_pose_output_loss: 0.6697 - val_emotion_output_loss: 0.9289 - val_gender_output_acc: 0.7863 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6300 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7419 - val_emotion_output_acc: 0.7072\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 7.06014\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 32s 89ms/step - loss: 6.2481 - gender_output_loss: 0.3353 - image_quality_output_loss: 0.9156 - age_output_loss: 1.2964 - weight_output_loss: 0.9026 - bag_output_loss: 0.7834 - footwear_output_loss: 0.7045 - pose_output_loss: 0.4840 - emotion_output_loss: 0.8263 - gender_output_acc: 0.8559 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4334 - weight_output_acc: 0.6445 - bag_output_acc: 0.6605 - footwear_output_acc: 0.6899 - pose_output_acc: 0.8034 - emotion_output_acc: 0.7119 - val_loss: 7.1424 - val_gender_output_loss: 0.4789 - val_image_quality_output_loss: 0.9471 - val_age_output_loss: 1.3840 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.8483 - val_footwear_output_loss: 0.8664 - val_pose_output_loss: 0.6977 - val_emotion_output_loss: 0.9344 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7308 - val_emotion_output_acc: 0.7016\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009800000465475021.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.06014\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d08db7390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUY9jYi1HxLq",
        "colab_type": "text"
      },
      "source": [
        "Saving the model as `person_multiclass_model_densenet_100_v1_frozen.h5`. <br> We've reached a val_loss of `7.06014`.\n",
        "\n",
        "Now's the time to unfreeze everything and try with higher resolution images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwxQ1OyngQFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in model.layers:\n",
        "    l.trainable = True\n",
        "\n",
        "check_all_layers_trainable(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGP-6-zVjTO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_100_v1_frozen.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5k3LVtktfb",
        "colab_type": "code",
        "outputId": "4341dc0a-6dc3-4b9d-ec7c-c8f453df1f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "compile_model(model)\n",
        "\n",
        "# for 50 epochs train on (200, 200, 3), with reduce_lr, early stopping, no data augmentation\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[reduce_lr_on_plateau, checkpoint, early_stopping]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "360/360 [==============================] - 254s 705ms/step - loss: 6.9499 - gender_output_loss: 0.4700 - image_quality_output_loss: 0.9283 - age_output_loss: 1.3754 - weight_output_loss: 0.9548 - bag_output_loss: 0.8539 - footwear_output_loss: 0.8093 - pose_output_loss: 0.6844 - emotion_output_loss: 0.8737 - gender_output_acc: 0.7766 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4029 - weight_output_acc: 0.6339 - bag_output_acc: 0.6071 - footwear_output_acc: 0.6390 - pose_output_acc: 0.7097 - emotion_output_acc: 0.7124 - val_loss: 6.9345 - val_gender_output_loss: 0.4449 - val_image_quality_output_loss: 0.9275 - val_age_output_loss: 1.3793 - val_weight_output_loss: 0.9659 - val_bag_output_loss: 0.8438 - val_footwear_output_loss: 0.8206 - val_pose_output_loss: 0.6353 - val_emotion_output_loss: 0.9172 - val_gender_output_acc: 0.7918 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6149 - val_footwear_output_acc: 0.6381 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.93446, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 199s 554ms/step - loss: 6.6379 - gender_output_loss: 0.4055 - image_quality_output_loss: 0.9119 - age_output_loss: 1.3474 - weight_output_loss: 0.9355 - bag_output_loss: 0.8284 - footwear_output_loss: 0.7680 - pose_output_loss: 0.5861 - emotion_output_loss: 0.8552 - gender_output_acc: 0.8180 - image_quality_output_acc: 0.5643 - age_output_acc: 0.4146 - weight_output_acc: 0.6352 - bag_output_acc: 0.6277 - footwear_output_acc: 0.6615 - pose_output_acc: 0.7537 - emotion_output_acc: 0.7127 - val_loss: 6.9425 - val_gender_output_loss: 0.4257 - val_image_quality_output_loss: 0.9261 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9628 - val_bag_output_loss: 0.8545 - val_footwear_output_loss: 0.8058 - val_pose_output_loss: 0.6283 - val_emotion_output_loss: 0.9266 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5701 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.6295 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7485 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 6.93446\n",
            "Epoch 3/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.4968 - gender_output_loss: 0.3764 - image_quality_output_loss: 0.9020 - age_output_loss: 1.3316 - weight_output_loss: 0.9293 - bag_output_loss: 0.8148 - footwear_output_loss: 0.7495 - pose_output_loss: 0.5467 - emotion_output_loss: 0.8464 - gender_output_acc: 0.8317 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4200 - weight_output_acc: 0.6381 - bag_output_acc: 0.6367 - footwear_output_acc: 0.6707 - pose_output_acc: 0.7757 - emotion_output_acc: 0.7121\n",
            "Epoch 00002: val_loss did not improve from 6.93446\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 200s 557ms/step - loss: 6.4966 - gender_output_loss: 0.3763 - image_quality_output_loss: 0.9018 - age_output_loss: 1.3320 - weight_output_loss: 0.9295 - bag_output_loss: 0.8146 - footwear_output_loss: 0.7493 - pose_output_loss: 0.5464 - emotion_output_loss: 0.8467 - gender_output_acc: 0.8315 - image_quality_output_acc: 0.5702 - age_output_acc: 0.4195 - weight_output_acc: 0.6379 - bag_output_acc: 0.6367 - footwear_output_acc: 0.6709 - pose_output_acc: 0.7759 - emotion_output_acc: 0.7120 - val_loss: 7.1647 - val_gender_output_loss: 0.5541 - val_image_quality_output_loss: 0.9594 - val_age_output_loss: 1.3939 - val_weight_output_loss: 0.9728 - val_bag_output_loss: 0.8702 - val_footwear_output_loss: 0.7981 - val_pose_output_loss: 0.6731 - val_emotion_output_loss: 0.9432 - val_gender_output_acc: 0.7586 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6079 - val_footwear_output_acc: 0.6386 - val_pose_output_acc: 0.7314 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.93446\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 199s 553ms/step - loss: 6.3807 - gender_output_loss: 0.3475 - image_quality_output_loss: 0.8915 - age_output_loss: 1.3173 - weight_output_loss: 0.9224 - bag_output_loss: 0.8024 - footwear_output_loss: 0.7372 - pose_output_loss: 0.5198 - emotion_output_loss: 0.8427 - gender_output_acc: 0.8476 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4238 - weight_output_acc: 0.6402 - bag_output_acc: 0.6502 - footwear_output_acc: 0.6796 - pose_output_acc: 0.7841 - emotion_output_acc: 0.7128 - val_loss: 6.9659 - val_gender_output_loss: 0.4025 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.4066 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8344 - val_footwear_output_loss: 0.8208 - val_pose_output_loss: 0.5775 - val_emotion_output_loss: 0.9677 - val_gender_output_acc: 0.8281 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.3841 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6537 - val_pose_output_acc: 0.7712 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00013999999646330252.\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.93446\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 200s 555ms/step - loss: 6.2015 - gender_output_loss: 0.3159 - image_quality_output_loss: 0.8799 - age_output_loss: 1.3002 - weight_output_loss: 0.9084 - bag_output_loss: 0.7834 - footwear_output_loss: 0.7135 - pose_output_loss: 0.4672 - emotion_output_loss: 0.8331 - gender_output_acc: 0.8679 - image_quality_output_acc: 0.5777 - age_output_acc: 0.4271 - weight_output_acc: 0.6398 - bag_output_acc: 0.6591 - footwear_output_acc: 0.6931 - pose_output_acc: 0.8112 - emotion_output_acc: 0.7127 - val_loss: 6.7297 - val_gender_output_loss: 0.3931 - val_image_quality_output_loss: 0.8936 - val_age_output_loss: 1.3647 - val_weight_output_loss: 0.9557 - val_bag_output_loss: 0.8144 - val_footwear_output_loss: 0.8002 - val_pose_output_loss: 0.5642 - val_emotion_output_loss: 0.9438 - val_gender_output_acc: 0.8241 - val_image_quality_output_acc: 0.5817 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.6537 - val_footwear_output_acc: 0.6608 - val_pose_output_acc: 0.7913 - val_emotion_output_acc: 0.7056\n",
            "\n",
            "Epoch 00005: val_loss improved from 6.93446 to 6.72972, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200.h5\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 200s 555ms/step - loss: 6.0937 - gender_output_loss: 0.2902 - image_quality_output_loss: 0.8718 - age_output_loss: 1.2792 - weight_output_loss: 0.9029 - bag_output_loss: 0.7700 - footwear_output_loss: 0.7090 - pose_output_loss: 0.4451 - emotion_output_loss: 0.8255 - gender_output_acc: 0.8748 - image_quality_output_acc: 0.5835 - age_output_acc: 0.4411 - weight_output_acc: 0.6445 - bag_output_acc: 0.6676 - footwear_output_acc: 0.6915 - pose_output_acc: 0.8228 - emotion_output_acc: 0.7132 - val_loss: 6.8786 - val_gender_output_loss: 0.4058 - val_image_quality_output_loss: 0.8924 - val_age_output_loss: 1.3920 - val_weight_output_loss: 0.9748 - val_bag_output_loss: 0.8321 - val_footwear_output_loss: 0.7742 - val_pose_output_loss: 0.6546 - val_emotion_output_loss: 0.9526 - val_gender_output_acc: 0.8231 - val_image_quality_output_acc: 0.5721 - val_age_output_acc: 0.3856 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6643 - val_pose_output_acc: 0.7631 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.72972\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 199s 554ms/step - loss: 6.0029 - gender_output_loss: 0.2692 - image_quality_output_loss: 0.8671 - age_output_loss: 1.2646 - weight_output_loss: 0.8945 - bag_output_loss: 0.7623 - footwear_output_loss: 0.6982 - pose_output_loss: 0.4225 - emotion_output_loss: 0.8244 - gender_output_acc: 0.8901 - image_quality_output_acc: 0.5865 - age_output_acc: 0.4449 - weight_output_acc: 0.6444 - bag_output_acc: 0.6728 - footwear_output_acc: 0.6954 - pose_output_acc: 0.8314 - emotion_output_acc: 0.7121 - val_loss: 6.8259 - val_gender_output_loss: 0.3956 - val_image_quality_output_loss: 0.8951 - val_age_output_loss: 1.4107 - val_weight_output_loss: 0.9577 - val_bag_output_loss: 0.8334 - val_footwear_output_loss: 0.7952 - val_pose_output_loss: 0.6057 - val_emotion_output_loss: 0.9325 - val_gender_output_acc: 0.8281 - val_image_quality_output_acc: 0.5786 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.6386 - val_footwear_output_acc: 0.6568 - val_pose_output_acc: 0.7732 - val_emotion_output_acc: 0.7041\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.72972\n",
            "Epoch 8/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8792 - gender_output_loss: 0.2473 - image_quality_output_loss: 0.8607 - age_output_loss: 1.2512 - weight_output_loss: 0.8822 - bag_output_loss: 0.7456 - footwear_output_loss: 0.6828 - pose_output_loss: 0.3948 - emotion_output_loss: 0.8145 - gender_output_acc: 0.9002 - image_quality_output_acc: 0.5937 - age_output_acc: 0.4508 - weight_output_acc: 0.6475 - bag_output_acc: 0.6831 - footwear_output_acc: 0.7062 - pose_output_acc: 0.8442 - emotion_output_acc: 0.7135\n",
            "Epoch 00007: val_loss did not improve from 6.72972\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 200s 554ms/step - loss: 5.8786 - gender_output_loss: 0.2469 - image_quality_output_loss: 0.8607 - age_output_loss: 1.2510 - weight_output_loss: 0.8818 - bag_output_loss: 0.7461 - footwear_output_loss: 0.6829 - pose_output_loss: 0.3947 - emotion_output_loss: 0.8144 - gender_output_acc: 0.9004 - image_quality_output_acc: 0.5936 - age_output_acc: 0.4507 - weight_output_acc: 0.6477 - bag_output_acc: 0.6830 - footwear_output_acc: 0.7063 - pose_output_acc: 0.8441 - emotion_output_acc: 0.7135 - val_loss: 6.9391 - val_gender_output_loss: 0.5340 - val_image_quality_output_loss: 0.9059 - val_age_output_loss: 1.3597 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8181 - val_footwear_output_loss: 0.7928 - val_pose_output_loss: 0.6063 - val_emotion_output_loss: 0.9474 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5852 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6386 - val_footwear_output_acc: 0.6537 - val_pose_output_acc: 0.7752 - val_emotion_output_acc: 0.7061\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.799999243114143e-05.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.72972\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 200s 555ms/step - loss: 5.6944 - gender_output_loss: 0.2197 - image_quality_output_loss: 0.8427 - age_output_loss: 1.2224 - weight_output_loss: 0.8668 - bag_output_loss: 0.7232 - footwear_output_loss: 0.6608 - pose_output_loss: 0.3560 - emotion_output_loss: 0.8029 - gender_output_acc: 0.9135 - image_quality_output_acc: 0.6017 - age_output_acc: 0.4630 - weight_output_acc: 0.6537 - bag_output_acc: 0.6970 - footwear_output_acc: 0.7117 - pose_output_acc: 0.8589 - emotion_output_acc: 0.7143 - val_loss: 6.8031 - val_gender_output_loss: 0.4032 - val_image_quality_output_loss: 0.8920 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.8114 - val_footwear_output_loss: 0.7935 - val_pose_output_loss: 0.5849 - val_emotion_output_loss: 0.9482 - val_gender_output_acc: 0.8296 - val_image_quality_output_acc: 0.5811 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6492 - val_footwear_output_acc: 0.6593 - val_pose_output_acc: 0.7913 - val_emotion_output_acc: 0.7051\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.72972\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 200s 554ms/step - loss: 5.5607 - gender_output_loss: 0.1992 - image_quality_output_loss: 0.8332 - age_output_loss: 1.2012 - weight_output_loss: 0.8567 - bag_output_loss: 0.7073 - footwear_output_loss: 0.6430 - pose_output_loss: 0.3252 - emotion_output_loss: 0.7949 - gender_output_acc: 0.9227 - image_quality_output_acc: 0.6039 - age_output_acc: 0.4747 - weight_output_acc: 0.6549 - bag_output_acc: 0.7030 - footwear_output_acc: 0.7168 - pose_output_acc: 0.8722 - emotion_output_acc: 0.7135 - val_loss: 7.0513 - val_gender_output_loss: 0.4290 - val_image_quality_output_loss: 0.8904 - val_age_output_loss: 1.4010 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.8297 - val_footwear_output_loss: 0.8072 - val_pose_output_loss: 0.7143 - val_emotion_output_loss: 1.0011 - val_gender_output_acc: 0.8412 - val_image_quality_output_acc: 0.5852 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.6497 - val_footwear_output_acc: 0.6638 - val_pose_output_acc: 0.7495 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.72972\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 200s 556ms/step - loss: 5.4361 - gender_output_loss: 0.1799 - image_quality_output_loss: 0.8271 - age_output_loss: 1.1762 - weight_output_loss: 0.8400 - bag_output_loss: 0.6943 - footwear_output_loss: 0.6352 - pose_output_loss: 0.2955 - emotion_output_loss: 0.7879 - gender_output_acc: 0.9302 - image_quality_output_acc: 0.6091 - age_output_acc: 0.4786 - weight_output_acc: 0.6566 - bag_output_acc: 0.7091 - footwear_output_acc: 0.7233 - pose_output_acc: 0.8866 - emotion_output_acc: 0.7147 - val_loss: 7.0040 - val_gender_output_loss: 0.4594 - val_image_quality_output_loss: 0.9091 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8305 - val_footwear_output_loss: 0.8102 - val_pose_output_loss: 0.6558 - val_emotion_output_loss: 0.9456 - val_gender_output_acc: 0.8367 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.6462 - val_footwear_output_acc: 0.6537 - val_pose_output_acc: 0.7873 - val_emotion_output_acc: 0.7006\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.859999266453087e-05.\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.72972\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 200s 555ms/step - loss: 5.2432 - gender_output_loss: 0.1482 - image_quality_output_loss: 0.8125 - age_output_loss: 1.1484 - weight_output_loss: 0.8176 - bag_output_loss: 0.6642 - footwear_output_loss: 0.6068 - pose_output_loss: 0.2723 - emotion_output_loss: 0.7730 - gender_output_acc: 0.9416 - image_quality_output_acc: 0.6179 - age_output_acc: 0.4948 - weight_output_acc: 0.6677 - bag_output_acc: 0.7259 - footwear_output_acc: 0.7378 - pose_output_acc: 0.8946 - emotion_output_acc: 0.7161 - val_loss: 7.0802 - val_gender_output_loss: 0.4607 - val_image_quality_output_loss: 0.9458 - val_age_output_loss: 1.3865 - val_weight_output_loss: 0.9500 - val_bag_output_loss: 0.8342 - val_footwear_output_loss: 0.8368 - val_pose_output_loss: 0.6948 - val_emotion_output_loss: 0.9714 - val_gender_output_acc: 0.8377 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6537 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.7651 - val_emotion_output_acc: 0.7026\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.72972\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d0ae3c470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE9ke2F7LWEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDH9czDnIos1",
        "colab_type": "text"
      },
      "source": [
        "Let's perform proper normalization, up until now I was using the `ImageDataGenerator`, method's normalizations, but now I've changed to calculating the mean and stddev on the complete dataset and normalizing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNE2FW3WhQn1",
        "colab_type": "code",
        "outputId": "c449d540-93e1-4270-dc93-13c65901edd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "compile_model(model)\n",
        "\n",
        "# for 50 epochs train on (200, 200, 3), with reduce_lr, early stopping, no data augmentation\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[reduce_lr_on_plateau, checkpoint, early_stopping]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "360/360 [==============================] - 251s 698ms/step - loss: 5.7250 - gender_output_loss: 0.2240 - image_quality_output_loss: 0.8542 - age_output_loss: 1.2293 - weight_output_loss: 0.8311 - bag_output_loss: 0.6838 - footwear_output_loss: 0.7377 - pose_output_loss: 0.3392 - emotion_output_loss: 0.8257 - gender_output_acc: 0.9097 - image_quality_output_acc: 0.5927 - age_output_acc: 0.4563 - weight_output_acc: 0.6675 - bag_output_acc: 0.7204 - footwear_output_acc: 0.6730 - pose_output_acc: 0.8757 - emotion_output_acc: 0.7135 - val_loss: 6.1829 - val_gender_output_loss: 0.2453 - val_image_quality_output_loss: 0.9001 - val_age_output_loss: 1.2741 - val_weight_output_loss: 0.8948 - val_bag_output_loss: 0.7365 - val_footwear_output_loss: 0.8112 - val_pose_output_loss: 0.4364 - val_emotion_output_loss: 0.8846 - val_gender_output_acc: 0.8973 - val_image_quality_output_acc: 0.5724 - val_age_output_acc: 0.4266 - val_weight_output_acc: 0.6498 - val_bag_output_acc: 0.6900 - val_footwear_output_acc: 0.6354 - val_pose_output_acc: 0.8542 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.18292, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 204s 567ms/step - loss: 5.6444 - gender_output_loss: 0.2071 - image_quality_output_loss: 0.8488 - age_output_loss: 1.2102 - weight_output_loss: 0.8164 - bag_output_loss: 0.6712 - footwear_output_loss: 0.7360 - pose_output_loss: 0.3308 - emotion_output_loss: 0.8240 - gender_output_acc: 0.9196 - image_quality_output_acc: 0.5972 - age_output_acc: 0.4672 - weight_output_acc: 0.6706 - bag_output_acc: 0.7247 - footwear_output_acc: 0.6751 - pose_output_acc: 0.8780 - emotion_output_acc: 0.7132 - val_loss: 6.1294 - val_gender_output_loss: 0.2699 - val_image_quality_output_loss: 0.8786 - val_age_output_loss: 1.2627 - val_weight_output_loss: 0.8977 - val_bag_output_loss: 0.7370 - val_footwear_output_loss: 0.7914 - val_pose_output_loss: 0.4195 - val_emotion_output_loss: 0.8727 - val_gender_output_acc: 0.8909 - val_image_quality_output_acc: 0.5933 - val_age_output_acc: 0.4405 - val_weight_output_acc: 0.6523 - val_bag_output_acc: 0.6974 - val_footwear_output_acc: 0.6577 - val_pose_output_acc: 0.8601 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.18292 to 6.12938, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized.h5\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 202s 561ms/step - loss: 5.6012 - gender_output_loss: 0.2058 - image_quality_output_loss: 0.8450 - age_output_loss: 1.2092 - weight_output_loss: 0.8095 - bag_output_loss: 0.6584 - footwear_output_loss: 0.7348 - pose_output_loss: 0.3196 - emotion_output_loss: 0.8188 - gender_output_acc: 0.9179 - image_quality_output_acc: 0.5998 - age_output_acc: 0.4697 - weight_output_acc: 0.6740 - bag_output_acc: 0.7254 - footwear_output_acc: 0.6756 - pose_output_acc: 0.8817 - emotion_output_acc: 0.7139 - val_loss: 6.2074 - val_gender_output_loss: 0.2475 - val_image_quality_output_loss: 0.8946 - val_age_output_loss: 1.2831 - val_weight_output_loss: 0.9106 - val_bag_output_loss: 0.7460 - val_footwear_output_loss: 0.8060 - val_pose_output_loss: 0.4336 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.9008 - val_image_quality_output_acc: 0.5828 - val_age_output_acc: 0.4345 - val_weight_output_acc: 0.6523 - val_bag_output_acc: 0.6930 - val_footwear_output_acc: 0.6344 - val_pose_output_acc: 0.8581 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.12938\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 204s 568ms/step - loss: 5.5569 - gender_output_loss: 0.2053 - image_quality_output_loss: 0.8434 - age_output_loss: 1.1958 - weight_output_loss: 0.8030 - bag_output_loss: 0.6556 - footwear_output_loss: 0.7210 - pose_output_loss: 0.3154 - emotion_output_loss: 0.8174 - gender_output_acc: 0.9189 - image_quality_output_acc: 0.6026 - age_output_acc: 0.4714 - weight_output_acc: 0.6734 - bag_output_acc: 0.7299 - footwear_output_acc: 0.6837 - pose_output_acc: 0.8818 - emotion_output_acc: 0.7131 - val_loss: 6.6726 - val_gender_output_loss: 0.2690 - val_image_quality_output_loss: 0.8926 - val_age_output_loss: 1.3412 - val_weight_output_loss: 0.9584 - val_bag_output_loss: 0.8015 - val_footwear_output_loss: 1.0270 - val_pose_output_loss: 0.4461 - val_emotion_output_loss: 0.9367 - val_gender_output_acc: 0.8924 - val_image_quality_output_acc: 0.5903 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6042 - val_pose_output_acc: 0.8403 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.12938\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 205s 570ms/step - loss: 5.5262 - gender_output_loss: 0.1997 - image_quality_output_loss: 0.8419 - age_output_loss: 1.1938 - weight_output_loss: 0.7948 - bag_output_loss: 0.6509 - footwear_output_loss: 0.7215 - pose_output_loss: 0.3068 - emotion_output_loss: 0.8168 - gender_output_acc: 0.9189 - image_quality_output_acc: 0.6010 - age_output_acc: 0.4777 - weight_output_acc: 0.6815 - bag_output_acc: 0.7310 - footwear_output_acc: 0.6823 - pose_output_acc: 0.8888 - emotion_output_acc: 0.7155 - val_loss: 6.3742 - val_gender_output_loss: 0.2750 - val_image_quality_output_loss: 0.9675 - val_age_output_loss: 1.2744 - val_weight_output_loss: 0.9263 - val_bag_output_loss: 0.7478 - val_footwear_output_loss: 0.8034 - val_pose_output_loss: 0.4788 - val_emotion_output_loss: 0.9011 - val_gender_output_acc: 0.8958 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.4385 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.6994 - val_footwear_output_acc: 0.6483 - val_pose_output_acc: 0.8462 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 6.12938\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 204s 567ms/step - loss: 5.4328 - gender_output_loss: 0.1864 - image_quality_output_loss: 0.8329 - age_output_loss: 1.1755 - weight_output_loss: 0.7836 - bag_output_loss: 0.6327 - footwear_output_loss: 0.7145 - pose_output_loss: 0.2956 - emotion_output_loss: 0.8115 - gender_output_acc: 0.9284 - image_quality_output_acc: 0.6033 - age_output_acc: 0.4782 - weight_output_acc: 0.6852 - bag_output_acc: 0.7415 - footwear_output_acc: 0.6813 - pose_output_acc: 0.8930 - emotion_output_acc: 0.7135 - val_loss: 6.2611 - val_gender_output_loss: 0.2536 - val_image_quality_output_loss: 0.8789 - val_age_output_loss: 1.2761 - val_weight_output_loss: 0.9061 - val_bag_output_loss: 0.7529 - val_footwear_output_loss: 0.8707 - val_pose_output_loss: 0.4353 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.9072 - val_image_quality_output_acc: 0.5863 - val_age_output_acc: 0.4286 - val_weight_output_acc: 0.6548 - val_bag_output_acc: 0.6944 - val_footwear_output_acc: 0.6339 - val_pose_output_acc: 0.8547 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.12938\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 205s 569ms/step - loss: 5.4014 - gender_output_loss: 0.1821 - image_quality_output_loss: 0.8306 - age_output_loss: 1.1783 - weight_output_loss: 0.7788 - bag_output_loss: 0.6230 - footwear_output_loss: 0.7071 - pose_output_loss: 0.2897 - emotion_output_loss: 0.8117 - gender_output_acc: 0.9291 - image_quality_output_acc: 0.6016 - age_output_acc: 0.4837 - weight_output_acc: 0.6850 - bag_output_acc: 0.7457 - footwear_output_acc: 0.6889 - pose_output_acc: 0.8946 - emotion_output_acc: 0.7134 - val_loss: 6.3148 - val_gender_output_loss: 0.2713 - val_image_quality_output_loss: 0.9376 - val_age_output_loss: 1.2725 - val_weight_output_loss: 0.9337 - val_bag_output_loss: 0.7581 - val_footwear_output_loss: 0.7946 - val_pose_output_loss: 0.4588 - val_emotion_output_loss: 0.8883 - val_gender_output_acc: 0.9028 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.4365 - val_weight_output_acc: 0.6528 - val_bag_output_acc: 0.6915 - val_footwear_output_acc: 0.6637 - val_pose_output_acc: 0.8547 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.12938\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.12938\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 205s 568ms/step - loss: 5.3592 - gender_output_loss: 0.1778 - image_quality_output_loss: 0.8307 - age_output_loss: 1.1656 - weight_output_loss: 0.7752 - bag_output_loss: 0.6110 - footwear_output_loss: 0.7069 - pose_output_loss: 0.2827 - emotion_output_loss: 0.8091 - gender_output_acc: 0.9329 - image_quality_output_acc: 0.6044 - age_output_acc: 0.4896 - weight_output_acc: 0.6850 - bag_output_acc: 0.7488 - footwear_output_acc: 0.6885 - pose_output_acc: 0.8946 - emotion_output_acc: 0.7145 - val_loss: 6.4713 - val_gender_output_loss: 0.2653 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3198 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.7596 - val_footwear_output_loss: 0.8623 - val_pose_output_loss: 0.4451 - val_emotion_output_loss: 0.9012 - val_gender_output_acc: 0.8958 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.4291 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6959 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8492 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.12938\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 204s 568ms/step - loss: 5.2684 - gender_output_loss: 0.1671 - image_quality_output_loss: 0.8229 - age_output_loss: 1.1453 - weight_output_loss: 0.7548 - bag_output_loss: 0.6062 - footwear_output_loss: 0.7049 - pose_output_loss: 0.2637 - emotion_output_loss: 0.8035 - gender_output_acc: 0.9365 - image_quality_output_acc: 0.6105 - age_output_acc: 0.4938 - weight_output_acc: 0.6896 - bag_output_acc: 0.7510 - footwear_output_acc: 0.6908 - pose_output_acc: 0.9029 - emotion_output_acc: 0.7151 - val_loss: 6.5502 - val_gender_output_loss: 0.2613 - val_image_quality_output_loss: 0.9299 - val_age_output_loss: 1.3444 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.7453 - val_footwear_output_loss: 0.9339 - val_pose_output_loss: 0.4457 - val_emotion_output_loss: 0.9145 - val_gender_output_acc: 0.8938 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.4162 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6920 - val_footwear_output_acc: 0.6037 - val_pose_output_acc: 0.8507 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.12938\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 204s 566ms/step - loss: 5.2280 - gender_output_loss: 0.1634 - image_quality_output_loss: 0.8211 - age_output_loss: 1.1396 - weight_output_loss: 0.7449 - bag_output_loss: 0.5958 - footwear_output_loss: 0.6992 - pose_output_loss: 0.2614 - emotion_output_loss: 0.8025 - gender_output_acc: 0.9377 - image_quality_output_acc: 0.6148 - age_output_acc: 0.4964 - weight_output_acc: 0.6982 - bag_output_acc: 0.7583 - footwear_output_acc: 0.6915 - pose_output_acc: 0.9009 - emotion_output_acc: 0.7156 - val_loss: 6.4416 - val_gender_output_loss: 0.2571 - val_image_quality_output_loss: 0.9442 - val_age_output_loss: 1.2905 - val_weight_output_loss: 0.9585 - val_bag_output_loss: 0.7655 - val_footwear_output_loss: 0.8772 - val_pose_output_loss: 0.4549 - val_emotion_output_loss: 0.8936 - val_gender_output_acc: 0.9033 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.4335 - val_weight_output_acc: 0.6523 - val_bag_output_acc: 0.6865 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.8547 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.12938\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 10/50\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f490e2ace80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9mMYJg0E6JU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRK0FQrsZg8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in backbone.layers:\n",
        "    l.trainable = False\n",
        "\n",
        "try:\n",
        "    check_all_layers_trainable(model)\n",
        "except AssertionError:\n",
        "    for l in model.layers:\n",
        "        print(l.name, l.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4yRlZAXZvIh",
        "colab_type": "code",
        "outputId": "1531865d-1027-4140-e0e8-9fe83843c290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "compile_model(model)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[reduce_lr_on_plateau, checkpoint, early_stopping]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.5702 - gender_output_loss: 0.2060 - image_quality_output_loss: 0.8404 - age_output_loss: 1.1948 - weight_output_loss: 0.8059 - bag_output_loss: 0.6548 - footwear_output_loss: 0.7319 - pose_output_loss: 0.3138 - emotion_output_loss: 0.8225 - gender_output_acc: 0.9197 - image_quality_output_acc: 0.6019 - age_output_acc: 0.4744 - weight_output_acc: 0.6773 - bag_output_acc: 0.7321 - footwear_output_acc: 0.6784 - pose_output_acc: 0.8861 - emotion_output_acc: 0.7141\n",
            "360/360 [==============================] - 183s 509ms/step - loss: 5.5681 - gender_output_loss: 0.2059 - image_quality_output_loss: 0.8399 - age_output_loss: 1.1948 - weight_output_loss: 0.8057 - bag_output_loss: 0.6544 - footwear_output_loss: 0.7317 - pose_output_loss: 0.3136 - emotion_output_loss: 0.8219 - gender_output_acc: 0.9198 - image_quality_output_acc: 0.6022 - age_output_acc: 0.4741 - weight_output_acc: 0.6773 - bag_output_acc: 0.7322 - footwear_output_acc: 0.6786 - pose_output_acc: 0.8862 - emotion_output_acc: 0.7144 - val_loss: 6.1222 - val_gender_output_loss: 0.2614 - val_image_quality_output_loss: 0.8823 - val_age_output_loss: 1.2627 - val_weight_output_loss: 0.8957 - val_bag_output_loss: 0.7391 - val_footwear_output_loss: 0.7876 - val_pose_output_loss: 0.4222 - val_emotion_output_loss: 0.8713 - val_gender_output_acc: 0.8929 - val_image_quality_output_acc: 0.5952 - val_age_output_acc: 0.4439 - val_weight_output_acc: 0.6478 - val_bag_output_acc: 0.7029 - val_footwear_output_acc: 0.6567 - val_pose_output_acc: 0.8576 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.12224, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized_frozen.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 161s 448ms/step - loss: 5.5427 - gender_output_loss: 0.2012 - image_quality_output_loss: 0.8430 - age_output_loss: 1.1957 - weight_output_loss: 0.8024 - bag_output_loss: 0.6520 - footwear_output_loss: 0.7196 - pose_output_loss: 0.3141 - emotion_output_loss: 0.8148 - gender_output_acc: 0.9213 - image_quality_output_acc: 0.6026 - age_output_acc: 0.4725 - weight_output_acc: 0.6753 - bag_output_acc: 0.7323 - footwear_output_acc: 0.6861 - pose_output_acc: 0.8831 - emotion_output_acc: 0.7133 - val_loss: 6.1214 - val_gender_output_loss: 0.2620 - val_image_quality_output_loss: 0.8821 - val_age_output_loss: 1.2609 - val_weight_output_loss: 0.8939 - val_bag_output_loss: 0.7402 - val_footwear_output_loss: 0.7876 - val_pose_output_loss: 0.4209 - val_emotion_output_loss: 0.8737 - val_gender_output_acc: 0.8938 - val_image_quality_output_acc: 0.5888 - val_age_output_acc: 0.4454 - val_weight_output_acc: 0.6483 - val_bag_output_acc: 0.7019 - val_footwear_output_acc: 0.6577 - val_pose_output_acc: 0.8571 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.12224 to 6.12136, saving model to /content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized_frozen.h5\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 163s 453ms/step - loss: 5.5220 - gender_output_loss: 0.1997 - image_quality_output_loss: 0.8367 - age_output_loss: 1.1937 - weight_output_loss: 0.8013 - bag_output_loss: 0.6523 - footwear_output_loss: 0.7191 - pose_output_loss: 0.3056 - emotion_output_loss: 0.8137 - gender_output_acc: 0.9220 - image_quality_output_acc: 0.6075 - age_output_acc: 0.4761 - weight_output_acc: 0.6794 - bag_output_acc: 0.7361 - footwear_output_acc: 0.6845 - pose_output_acc: 0.8889 - emotion_output_acc: 0.7124 - val_loss: 6.1244 - val_gender_output_loss: 0.2623 - val_image_quality_output_loss: 0.8840 - val_age_output_loss: 1.2614 - val_weight_output_loss: 0.8947 - val_bag_output_loss: 0.7396 - val_footwear_output_loss: 0.7890 - val_pose_output_loss: 0.4202 - val_emotion_output_loss: 0.8732 - val_gender_output_acc: 0.8943 - val_image_quality_output_acc: 0.5873 - val_age_output_acc: 0.4449 - val_weight_output_acc: 0.6493 - val_bag_output_acc: 0.6984 - val_footwear_output_acc: 0.6587 - val_pose_output_acc: 0.8566 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.12136\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 165s 457ms/step - loss: 5.5187 - gender_output_loss: 0.1972 - image_quality_output_loss: 0.8389 - age_output_loss: 1.1896 - weight_output_loss: 0.8022 - bag_output_loss: 0.6528 - footwear_output_loss: 0.7202 - pose_output_loss: 0.3058 - emotion_output_loss: 0.8120 - gender_output_acc: 0.9230 - image_quality_output_acc: 0.6049 - age_output_acc: 0.4782 - weight_output_acc: 0.6778 - bag_output_acc: 0.7341 - footwear_output_acc: 0.6857 - pose_output_acc: 0.8856 - emotion_output_acc: 0.7144 - val_loss: 6.1295 - val_gender_output_loss: 0.2635 - val_image_quality_output_loss: 0.8840 - val_age_output_loss: 1.2640 - val_weight_output_loss: 0.8916 - val_bag_output_loss: 0.7396 - val_footwear_output_loss: 0.7872 - val_pose_output_loss: 0.4254 - val_emotion_output_loss: 0.8742 - val_gender_output_acc: 0.8943 - val_image_quality_output_acc: 0.5878 - val_age_output_acc: 0.4415 - val_weight_output_acc: 0.6503 - val_bag_output_acc: 0.6989 - val_footwear_output_acc: 0.6562 - val_pose_output_acc: 0.8586 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 5.999999848427251e-05.\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.12136\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 163s 452ms/step - loss: 5.5084 - gender_output_loss: 0.1927 - image_quality_output_loss: 0.8374 - age_output_loss: 1.1922 - weight_output_loss: 0.8012 - bag_output_loss: 0.6495 - footwear_output_loss: 0.7135 - pose_output_loss: 0.3081 - emotion_output_loss: 0.8139 - gender_output_acc: 0.9260 - image_quality_output_acc: 0.6035 - age_output_acc: 0.4778 - weight_output_acc: 0.6773 - bag_output_acc: 0.7374 - footwear_output_acc: 0.6877 - pose_output_acc: 0.8844 - emotion_output_acc: 0.7149 - val_loss: 6.1280 - val_gender_output_loss: 0.2647 - val_image_quality_output_loss: 0.8822 - val_age_output_loss: 1.2630 - val_weight_output_loss: 0.8926 - val_bag_output_loss: 0.7400 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.4250 - val_emotion_output_loss: 0.8727 - val_gender_output_acc: 0.8934 - val_image_quality_output_acc: 0.5913 - val_age_output_acc: 0.4410 - val_weight_output_acc: 0.6513 - val_bag_output_acc: 0.6994 - val_footwear_output_acc: 0.6577 - val_pose_output_acc: 0.8576 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 6.12136\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 166s 460ms/step - loss: 5.5148 - gender_output_loss: 0.1992 - image_quality_output_loss: 0.8395 - age_output_loss: 1.1943 - weight_output_loss: 0.8026 - bag_output_loss: 0.6501 - footwear_output_loss: 0.7132 - pose_output_loss: 0.3019 - emotion_output_loss: 0.8141 - gender_output_acc: 0.9238 - image_quality_output_acc: 0.6024 - age_output_acc: 0.4705 - weight_output_acc: 0.6747 - bag_output_acc: 0.7344 - footwear_output_acc: 0.6864 - pose_output_acc: 0.8878 - emotion_output_acc: 0.7147 - val_loss: 6.1257 - val_gender_output_loss: 0.2614 - val_image_quality_output_loss: 0.8822 - val_age_output_loss: 1.2636 - val_weight_output_loss: 0.8929 - val_bag_output_loss: 0.7404 - val_footwear_output_loss: 0.7868 - val_pose_output_loss: 0.4250 - val_emotion_output_loss: 0.8734 - val_gender_output_acc: 0.8948 - val_image_quality_output_acc: 0.5938 - val_age_output_acc: 0.4380 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.7004 - val_footwear_output_acc: 0.6587 - val_pose_output_acc: 0.8576 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.12136\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 165s 458ms/step - loss: 5.5253 - gender_output_loss: 0.1975 - image_quality_output_loss: 0.8397 - age_output_loss: 1.1932 - weight_output_loss: 0.8015 - bag_output_loss: 0.6480 - footwear_output_loss: 0.7171 - pose_output_loss: 0.3126 - emotion_output_loss: 0.8158 - gender_output_acc: 0.9222 - image_quality_output_acc: 0.6023 - age_output_acc: 0.4753 - weight_output_acc: 0.6775 - bag_output_acc: 0.7327 - footwear_output_acc: 0.6844 - pose_output_acc: 0.8855 - emotion_output_acc: 0.7135 - val_loss: 6.1251 - val_gender_output_loss: 0.2629 - val_image_quality_output_loss: 0.8804 - val_age_output_loss: 1.2629 - val_weight_output_loss: 0.8929 - val_bag_output_loss: 0.7416 - val_footwear_output_loss: 0.7873 - val_pose_output_loss: 0.4246 - val_emotion_output_loss: 0.8726 - val_gender_output_acc: 0.8938 - val_image_quality_output_acc: 0.5908 - val_age_output_acc: 0.4400 - val_weight_output_acc: 0.6523 - val_bag_output_acc: 0.6989 - val_footwear_output_acc: 0.6562 - val_pose_output_acc: 0.8571 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.599999909056351e-05.\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.12136\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 164s 456ms/step - loss: 5.4944 - gender_output_loss: 0.1915 - image_quality_output_loss: 0.8390 - age_output_loss: 1.1893 - weight_output_loss: 0.8005 - bag_output_loss: 0.6420 - footwear_output_loss: 0.7132 - pose_output_loss: 0.3064 - emotion_output_loss: 0.8125 - gender_output_acc: 0.9280 - image_quality_output_acc: 0.6037 - age_output_acc: 0.4772 - weight_output_acc: 0.6762 - bag_output_acc: 0.7391 - footwear_output_acc: 0.6862 - pose_output_acc: 0.8872 - emotion_output_acc: 0.7138 - val_loss: 6.1232 - val_gender_output_loss: 0.2623 - val_image_quality_output_loss: 0.8815 - val_age_output_loss: 1.2622 - val_weight_output_loss: 0.8922 - val_bag_output_loss: 0.7418 - val_footwear_output_loss: 0.7875 - val_pose_output_loss: 0.4232 - val_emotion_output_loss: 0.8725 - val_gender_output_acc: 0.8943 - val_image_quality_output_acc: 0.5888 - val_age_output_acc: 0.4405 - val_weight_output_acc: 0.6518 - val_bag_output_acc: 0.7004 - val_footwear_output_acc: 0.6572 - val_pose_output_acc: 0.8576 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.12136\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 164s 455ms/step - loss: 5.4908 - gender_output_loss: 0.1900 - image_quality_output_loss: 0.8407 - age_output_loss: 1.1840 - weight_output_loss: 0.7969 - bag_output_loss: 0.6403 - footwear_output_loss: 0.7155 - pose_output_loss: 0.3066 - emotion_output_loss: 0.8167 - gender_output_acc: 0.9243 - image_quality_output_acc: 0.6015 - age_output_acc: 0.4765 - weight_output_acc: 0.6840 - bag_output_acc: 0.7390 - footwear_output_acc: 0.6885 - pose_output_acc: 0.8903 - emotion_output_acc: 0.7154 - val_loss: 6.1248 - val_gender_output_loss: 0.2617 - val_image_quality_output_loss: 0.8815 - val_age_output_loss: 1.2627 - val_weight_output_loss: 0.8935 - val_bag_output_loss: 0.7413 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.4238 - val_emotion_output_loss: 0.8724 - val_gender_output_acc: 0.8948 - val_image_quality_output_acc: 0.5913 - val_age_output_acc: 0.4405 - val_weight_output_acc: 0.6513 - val_bag_output_acc: 0.7014 - val_footwear_output_acc: 0.6572 - val_pose_output_acc: 0.8581 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.12136\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb1a1c4470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkXJpkCYJH0j",
        "colab_type": "text"
      },
      "source": [
        "Some common methods, for doing the regular stuff again and again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jmHvnMYMDJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13CWq2jCUbVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPXDt8loVEhT",
        "colab_type": "code",
        "outputId": "ee4ad9be-c358-42dd-dc77-aaee5d9da0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "show_results(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 14s 228ms/step\n",
            "                metric_name  accuracy\n",
            "0         gender_output_acc  0.892857\n",
            "1  image_quality_output_acc  0.595238\n",
            "2            age_output_acc  0.443948\n",
            "3         weight_output_acc  0.647817\n",
            "4            bag_output_acc  0.702877\n",
            "5       footwear_output_acc  0.656746\n",
            "6           pose_output_acc  0.857639\n",
            "7        emotion_output_acc  0.704365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsY65bYZV-Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized_frozen.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0043rc_JPBy",
        "colab_type": "code",
        "outputId": "035e9a43-a72e-4442-df9f-d663b1236085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "compile_model(model)\n",
        "model.load_weights('/content/gdrive/My Drive/Person_MultiClass/person_multiclass_model_densenet_200_normalized_frozen_augmented.h5')\n",
        "show_results(model, show_loss=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 16s 249ms/step\n",
            "                  metric_name  accuracy\n",
            "0                        loss  6.113962\n",
            "1          gender_output_loss  0.267376\n",
            "2   image_quality_output_loss  0.883270\n",
            "3             age_output_loss  1.261447\n",
            "4          weight_output_loss  0.901494\n",
            "5             bag_output_loss  0.746320\n",
            "6        footwear_output_loss  0.760049\n",
            "7            pose_output_loss  0.419371\n",
            "8         emotion_output_loss  0.874636\n",
            "9           gender_output_acc  0.888889\n",
            "10   image_quality_output_acc  0.584821\n",
            "11             age_output_acc  0.447421\n",
            "12          weight_output_acc  0.652778\n",
            "13             bag_output_acc  0.697917\n",
            "14        footwear_output_acc  0.670635\n",
            "15            pose_output_acc  0.857143\n",
            "16         emotion_output_acc  0.704365\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}